---
title: Solving Big AI’s Big Energy Problem
date: "2021-03-16 18:02:13"
author: Tech - The Next Web
authorlink: https://thenextweb.com/neural/2021/03/16/solving-big-ais-big-energy-problem/
tags:
- Tech-The-Next-Web
---
<img src="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2015/05/artificialintelligence-1200x673.jpg" width="1200" height="673"><br />It seems that the more ground-breaking deep learning models are in AI, the more massive they get. This summer’s most buzzed-about model for natural language processing, GPT-3, is a perfect example. To reach the levels of accuracy and speed to write like a human, the model needed 175 billion parameters, 350 GB of memory and $12 million to train (think of training as the “learning” phase). But, beyond cost alone, big AI models like this have a big energy problem.  UMass Amherst researchers found that the computing power needed to train a large AI model can produce over 600,000 pounds&#8230; <br><br><a href="https://thenextweb.com/neural/2021/03/16/solving-big-ais-big-energy-problem/?utm_source=social&#038;utm_medium=feed&#038;utm_campaign=profeed">This story continues</a> at The Next Web